{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814649e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3023d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5] END ........................alpha=0.05;, score=0.681 total time=13.7min\n",
      "[CV 2/5] END ........................alpha=0.05;, score=0.681 total time=14.5min\n",
      "[CV 3/5] END ........................alpha=0.05;, score=0.715 total time=14.5min\n",
      "[CV 4/5] END ........................alpha=0.05;, score=0.699 total time=15.6min\n",
      "[CV 5/5] END ........................alpha=0.05;, score=0.682 total time=15.7min\n",
      "[CV 1/5] END .........................alpha=0.5;, score=0.666 total time=14.3min\n",
      "[CV 2/5] END .........................alpha=0.5;, score=0.682 total time=14.4min\n",
      "[CV 3/5] END .........................alpha=0.5;, score=0.705 total time=13.7min\n",
      "[CV 4/5] END .........................alpha=0.5;, score=0.690 total time=14.9min\n",
      "[CV 5/5] END .........................alpha=0.5;, score=0.679 total time=14.7min\n",
      "[CV 1/5] END ........................alpha=0.95;, score=0.661 total time=15.1min\n",
      "[CV 2/5] END ........................alpha=0.95;, score=0.670 total time=16.1min\n",
      "[CV 3/5] END ........................alpha=0.95;, score=0.708 total time=15.2min\n",
      "[CV 4/5] END ........................alpha=0.95;, score=0.691 total time=16.5min\n",
      "[CV 5/5] END ........................alpha=0.95;, score=0.693 total time=15.8min\n",
      "Balanced accuracy: 0.686\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import create_training\n",
    "import json\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glmnet import LogitNet\n",
    "\n",
    "MIN_TRIALS_PER_CLASS = 5\n",
    "\n",
    "def compute_weights_by_class(target):\n",
    "    # Compute weights for each target class. This is especially important for confidence data, which are highly unbalanced.\n",
    "    # For example if we had 2 classes and there were 50 trials of class 0 and 100 trials of class 2, then the weights would\n",
    "    # be (0.666, 0.333), i.e. the class with half as many trials would be weighted twice as much when fitting the network.\n",
    "    utarget_sub = np.unique(target)\n",
    "    weights = class_weight.compute_class_weight(\"balanced\", classes=utarget_sub, y=target)\n",
    "    weights = dict(zip(utarget_sub, weights/np.sum(weights)))\n",
    "    return weights\n",
    "\n",
    "def compute_weights_by_sample(target):\n",
    "    # Like compute_weights_by_class, but return a weight for each sample.\n",
    "    weights_class = compute_weights_by_class(target)\n",
    "    weights_sample = np.full((len(target)), np.nan)\n",
    "    for i, t in enumerate(target):\n",
    "        weights_sample[i] = weights_class[t]\n",
    "    return weights_sample\n",
    "\n",
    "def train_conv(target):\n",
    "    # Train on the raw data (not dim reduced).\n",
    "    data = np.load(\"train.npz\", allow_pickle=True)\n",
    "    with open(\"train_split.json\", \"r\") as f:\n",
    "        split = json.load(f)\n",
    "\n",
    "    # The number of output neurons at the last layer is equal to the number of categories we're classifying into.\n",
    "    if target == \"resp\":\n",
    "        noutput = 2\n",
    "    elif target == \"conf\":\n",
    "        noutput = 3\n",
    "    elif target == \"condition\":\n",
    "        noutput = 5\n",
    "\n",
    "    nsub = np.max(data['sub']) + 1\n",
    "\n",
    "    scores = np.full((nsub), np.nan)\n",
    "    for sno in range(nsub):\n",
    "        # Find this participant's data.\n",
    "        input_sub = data['input'][data['sub'] == sno, :, :]\n",
    "        target_sub = data[target][data['sub'] == sno]\n",
    "\n",
    "        # Convert string labels to int.\n",
    "        if target == \"condition\":\n",
    "            _, target_sub = np.unique(target_sub, return_inverse=True)\n",
    "\n",
    "        input_sub_train = input_sub[split['all_idx_train'][sno], :, :]\n",
    "        input_sub_valid = input_sub[split['all_idx_valid'][sno], :, :]\n",
    "        target_sub_train = target_sub[split['all_idx_train'][sno]]\n",
    "        target_sub_valid = target_sub[split['all_idx_valid'][sno]]\n",
    "\n",
    "        if np.unique(target_sub_train).size < noutput or np.any(np.unique(target_sub_train, return_counts=True)[1] < MIN_TRIALS_PER_CLASS):\n",
    "            continue\n",
    "\n",
    "        # Standardize predictors.\n",
    "        train_shape = input_sub_train.shape\n",
    "        valid_shape = input_sub_valid.shape\n",
    "        input_sub_train = np.reshape(input_sub_train, (input_sub_train.shape[0], input_sub_train.shape[1]*input_sub_train.shape[2]), order=\"C\")\n",
    "        input_sub_valid = np.reshape(input_sub_valid, (input_sub_valid.shape[0], input_sub_valid.shape[1]*input_sub_valid.shape[2]), order=\"C\")\n",
    "        scale = StandardScaler()\n",
    "        scale = scale.fit(input_sub_train)\n",
    "        input_sub_train = scale.transform(input_sub_train)\n",
    "        input_sub_valid = scale.transform(input_sub_valid)\n",
    "        input_sub_train = np.reshape(input_sub_train, train_shape, order=\"C\")\n",
    "        input_sub_valid = np.reshape(input_sub_valid, valid_shape, order=\"C\")\n",
    "\n",
    "        input_sub_train[np.isnan(input_sub_train)] = 0\n",
    "        input_sub_valid[np.isnan(input_sub_valid)] = 0\n",
    "\n",
    "        weights = compute_weights_by_class(target_sub_train)\n",
    "        weights_valid_sample = compute_weights_by_sample(target_sub_valid)\n",
    "\n",
    "        # Build the network.\n",
    "        inputs = keras.Input(shape=input_sub.shape[1:3])\n",
    "        x = inputs\n",
    "        x = keras.layers.Conv1D(filters=5, kernel_size=9, activation=\"relu\")(x)  \n",
    "        x = keras.layers.MaxPooling1D(3)(x) \n",
    "        x = keras.layers.Conv1D(filters=5, kernel_size=9, activation=\"relu\")(x)\n",
    "        x = keras.layers.MaxPooling1D(3)(x)\n",
    "        x = keras.layers.Conv1D(filters=5, kernel_size=9, activation=\"relu\")(x)\n",
    "        x = keras.layers.MaxPooling1D(3)(x)\n",
    "        x = keras.layers.Flatten()(x)\n",
    "        x = keras.layers.Dense(30, activation=\"relu\")(x) \n",
    "        x = keras.layers.Dropout(0.5)(x)\n",
    "        outputs = keras.layers.Dense(noutput, activation=\"softmax\")(x)\n",
    "\n",
    "        # Fit the network and store weighted accuracy for the validation data from the last epoch. \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.SparseCategoricalCrossentropy(), weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "        history = model.fit(x=input_sub_train, y=target_sub_train, verbose=2, class_weight=weights, validation_data=(input_sub_valid, target_sub_valid, weights_valid_sample), batch_size=32, epochs=100)\n",
    "        scores[sno] = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "\n",
    "    print('Balanced accuracy (N=%d): %.3f (%.3f)' % (np.sum(~np.isnan(scores)), np.nanmean(scores), np.nanstd(scores)))\n",
    "\n",
    "def test_classifier(classifier, target, file_train, normalize):\n",
    "    data = np.load(file_train, allow_pickle=True)\n",
    "    with open(\"train_split.json\", \"r\") as f:\n",
    "        split = json.load(f)\n",
    "\n",
    "    input_key = 'input' if 'input' in data else 'reduced_input'\n",
    "    input = data[input_key]\n",
    "    if input_key == 'input':\n",
    "        input = np.moveaxis(input, -1, 1)\n",
    "        input = np.reshape(input, (input.shape[0], input.shape[1]*input.shape[2]), order=\"C\")\n",
    "\n",
    "    nsub = np.max(data['sub']) + 1\n",
    "\n",
    "    scores = np.full((nsub), np.nan)\n",
    "    for sno in range(nsub):\n",
    "        input_sub = input[data['sub'] == sno, :]\n",
    "        target_sub = data[target][data['sub'] == sno]\n",
    "\n",
    "        input_sub_train = input_sub[split['all_idx_train'][sno], :]\n",
    "        input_sub_valid = input_sub[split['all_idx_valid'][sno], :]\n",
    "        target_sub_train = target_sub[split['all_idx_train'][sno]]\n",
    "        target_sub_valid = target_sub[split['all_idx_valid'][sno]]\n",
    "\n",
    "        if normalize:\n",
    "            scale = StandardScaler()\n",
    "            scale = scale.fit(input_sub_train)\n",
    "            input_sub_train = scale.transform(input_sub_train)\n",
    "            input_sub_valid = scale.transform(input_sub_valid)\n",
    "\n",
    "        input_sub_train[np.isnan(input_sub_train)] = 0\n",
    "        input_sub_valid[np.isnan(input_sub_valid)] = 0\n",
    "\n",
    "        if target == \"resp\":\n",
    "            noutput = 2\n",
    "        elif target == \"conf\":\n",
    "            noutput = 3\n",
    "        elif target == \"condition\":\n",
    "            noutput = 5\n",
    "        if np.unique(target_sub_train).size < noutput or np.any(np.unique(target_sub_train, return_counts=True)[1] < MIN_TRIALS_PER_CLASS):\n",
    "            continue\n",
    "\n",
    "        weights = compute_weights_by_sample(target_sub_train)\n",
    "\n",
    "        model = classifier.fit(input_sub_train, target_sub_train, weights)\n",
    "        scores[sno] = balanced_accuracy_score(target_sub_valid, model.predict(input_sub_valid))\n",
    "\n",
    "    print('Balanced accuracy (N=%d): %.3f (%.3f)' % (np.sum(~np.isnan(scores)), np.nanmean(scores), np.nanstd(scores)))\n",
    "\n",
    "def random_forest(target, file_train):\n",
    "    test_classifier(RandomForestClassifier(n_jobs=64, n_estimators=100, max_depth=None), target, file_train, False)\n",
    "\n",
    "def boosting(target, file_train, n_estimators=100, max_depth=3):\n",
    "    test_classifier(GradientBoostingClassifier(n_estimators=n_estimators, max_depth=max_depth), target, file_train, False)\n",
    "\n",
    "def svmc(target, file_train, kernel, C=1):\n",
    "    test_classifier(svm.SVC(kernel=kernel, C=C), target, file_train, True)\n",
    "\n",
    "def elastic_net(target, file_train):\n",
    "    data = np.load(file_train, allow_pickle=True)\n",
    "\n",
    "    input_key = 'input' if 'input' in data else 'reduced_input'\n",
    "    input = data[input_key]\n",
    "    if input_key == 'input':\n",
    "        input = np.moveaxis(input, -1, 1)\n",
    "        input = np.reshape(input, (input.shape[0], input.shape[1]*input.shape[2]), order=\"C\")\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(input, data[target], test_size=0.2)\n",
    "\n",
    "    scale = StandardScaler()\n",
    "    scale = scale.fit(X_train)\n",
    "    X_train = scale.transform(X_train)\n",
    "    X_valid = scale.transform(X_valid)\n",
    "\n",
    "    X_train[np.isnan(X_train)] = 0\n",
    "    X_valid[np.isnan(X_valid)] = 0\n",
    "\n",
    "    weights = compute_weights_by_sample(y_train)\n",
    "\n",
    "\n",
    "    # create LogitNet object\n",
    "    logitnet = LogitNet(alpha=1)\n",
    "    \n",
    "    # create StratifiedKFold object\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    # create GridSearchCV\n",
    "    classifier = GridSearchCV(logitnet, {\"alpha\": [0.05, 0.5, 0.95]}, scoring=\"balanced_accuracy\", cv=skf, verbose=3)\n",
    "\n",
    "    model = classifier.fit(X_train, y_train, sample_weight=weights)\n",
    "    prediction = model.predict(X_valid)\n",
    "\n",
    "    print('Balanced accuracy: %0.3f' % balanced_accuracy_score(y_valid, prediction))\n",
    "\n",
    "elastic_net(\"resp\", \"train.npz\")                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa4c96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
